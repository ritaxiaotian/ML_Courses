
https://www.csie.ntu.edu.tw/~htlin/mooc/

https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf

https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/info

https://www.coursera.org/learn/ntumlone-algorithmicfoundations

# Week 1 The Learning Problem

    1. When machine learning
            1.1 exists some 'underlying pattern to be learned' - so that performance measure can be improved
            
            1.2 but no programmable (easy) definition - so that ML is needed
            
            1.3 somehow there is data about the pattern - so that ML has some inputs to learn from

    2. Why machine learning

    3. How can machine learning

    4. How can machine learn better
    
Components of machine learning
    
    1. Input
    
    2. Output
    
    3. unknown pattern to be learned    
    
    4. data: training examples  
    
    5. hypothesis: skill with hopefully good performance    
    

# Week 2 Learning to Answer Yes/No

    1. Perceptron Hyothesis Set      
    
        Credit Approval Problem: A Simple Hypothesis Set: "The Perceptron"

        线性分类器
    
    2. Perceptron Learning Algorithm (PLA)
    
        选一条最合适的线性分类器

        want g = f on known dataset

        W(t+1) = W(t) + y*x    
    
        利用向量计算,判断正确的perceptron
        
        如果不会停下来??
        
        在一定情况下,这个算法一定会停下来
    
    3. Guarantee of PLA
    
        If PLA halts, D allows some w to make no mistake; call such D linear seperable
        
        Linearly seperable


June 1 2018

    4. Non-seperable Data
    
        PLA: simple to implement, fast, works in any dimension
        
        Assumes linear seperable D to halt; not fully sure how long halting takes (不知道是否线性可分:可能不会停下; 不知道什么时候才会停下)
        
        Learning with Noisy Data : Line with Noise Tolerance
        
        Pocket Algorithm: Modify PLA by keeping best weights in pocket
        
        Since we do no know whether D is linear seperable in advance, we may decide to just go with pocket instead of PLA.
        
        When D is linear seperable: 
        
            pocket on D is slower than PLA for two reasons:
            
                1. pocket要花时间存起来weights
                
                2. 对于每一条线,pocket需要检查所有的点的误差       
               

# Week 3 Types of Learning

    1. Learning with different Output Space
    
        是非题（二元分类）：
        
        • credit approve/disapprove
        • email spam/non-spam
        • patient sick/not sick
        • ad profitable/not profitable
        • answer correct/incorrect (KDDCup 2010)
        
      core and important problem with many tools as building block of other tools
        
      Multiclass Classification Problems
        
            • written digits ⇒ 0, 1, · · · , 9
            • pictures ⇒ apple, orange, strawberry
            • emails ⇒ spam, primary, social, promotion, update (Google)
            
     many applications in practice, especially for ‘recognition’
       
     Regression
       
        • binary classification: patient features ⇒ sick or not
        • multiclass classification: patient features ⇒ which type of cancer
        • regression: patient features ⇒ how many days before recovery
        • Y = R or Y = [lower, upper] ⊂ R (bounded regression)
        —deeply studied in statistics
        • company data ⇒ stock price
        • climate data ⇒ temperature
        
        also core and important with many ‘statistical’ tools as building block of other tools
        
        Structured Learning: Sequence Tagging Problem：
        
        • multiclass classification: word ⇒ word class
        • structured learning: sentence ⇒ structure (class of each word)
        • Y = {PVN, PVP, NVN, PV , · · · }, not including VVVVV
        • huge multiclass classification problem (structure ≡ hyperclass) without ‘explicit’ class definition
        • protein data ⇒ protein folding
        • speech data ⇒ speech parse tree
        
        a fancy but complicated learning problem
        
            1 binary classification
            2 multiclass classification
            3 regression
            4 structured learning

    2. Learning with different data label
    
        Supervised: Coin Recognition Revisited
        
        Unsupervised: Coin Recognition without yn
        
            unsupervised multiclass classification ⇐⇒ ‘clustering’
            
            Other Clustering Problems
                • articles ⇒ topics
                • consumer profiles ⇒ consumer groups
    
        Other Unsupervised Learning Problems
            • clustering: {x n } ⇒ cluster(x)
            (≈ ‘unsupervised multiclass classification’)
            —i.e. articles ⇒ topics
            • density estimation: {x n } ⇒ density(x)
            (≈ ‘unsupervised bounded regression’)
            —i.e. traffic reports with location ⇒ dangerous areas
            • outlier detection: {x n } ⇒ unusual(x)
            (≈ extreme ‘unsupervised binary classification’)
            —i.e. Internet logs ⇒ intrusion alert
            • . . . and a lot more!!
        unsupervised learning: diverse, with possibly very different performance goals
        
        Semi-supervised: Coin Recognition with Some y n
        
        Other Semi-supervised Learning Problems
            • face images with a few labeled ⇒ face identifier (Facebook)
            • medicine data with a few labeled ⇒ medicine effect predictor
            
        semi-supervised learning: leverage unlabeled data to avoid ‘expensive’ labeling
        
        Reinforcement Learning
        
            Other Reinforcement Learning Problems Using (x, ỹ , goodness)
                • (customer, ad choice, ad click earning) ⇒ ad system
                • (cards, strategy, winning amount) ⇒ black jack agent
                                
        Learning with Different Data Label y n
            • supervised: all y n
            • unsupervised: no y n
            • semi-supervised: some y n
            • reinforcement: implicit y n by goodness( ỹ n )
            • . . . and more!!
    
    3. learning with different protocol
    
        Batch Learning: Coin Recognition Revisited
        
        batch supervised multiclass classification: learn from all known data
        
            • batch of (email, spam?) ⇒ spam filter
            • batch of (patient, cancer) ⇒ cancer classifier
            • batch of patient data ⇒ group of patients
        
        batch learning: a very common protocol
        
        • batch spam filter: learn with known (email, spam?) pairs, and predict with fixed g
        • online spam filter, which sequentially:        
            1 observe an email x t
            2 predict spam status with current g t (x t )
            3 receive ‘desired label’ y t from user, and then update g t with (x t , y t )
            
        Connection to What We Have Learned
            • PLA can be easily adapted to online protocol (how?)
            • reinforcement learning is often done online (why?)
            
        Protocol ⇔ Learning Philosophy
            • batch: ‘duck feeding’ unknown target function
            • online: ‘passive sequential’
            • active: ‘question asking’ (sequentially)
            —query the y n of the chosen xn
    
        Learning with Different Protocol f ⇒ (x n , y n )
            • batch: all known data unknown target function
            • online: sequential (passive) data
            • active: strategically-observed data
            • . . . and more!!
            
    4. learning with different input space
    
        More on Concrete Features
        
            • (size, mass) for coin classification
            • customer info for credit approval
            • patient info for cancer diagnosis
            • often including ‘human intelligence’ on the learning task
        
        concrete features: the ‘easy’ ones for ML
        
        Raw Features: Digit Recognition Problem
        
            • image pixels, speech signal, etc.
        
        raw features: often need human or machines to convert to concrete ones
        
        Abstract Features: Rating Prediction Problem
        
        Rating Prediction Problem (KDDCup 2011)
            • given previous (userid, itemid, rating) tuples, predict the rating that some userid would give to itemid?
            • a regression problem with Y ⊆ R as rating and X ⊆ N × N as (userid, itemid)
            • ‘no physical meaning’; thus even more difficult for ML Other Problems with Abstract Features
            • student ID in online tutoring system (KDDCup 2010)
            • advertisement ID in online ad system

    
# Week 4 Feasibility of Learning

    1. Learning is impossible?
    
    2. Probability to the rescue
    
    3.Connection to Learning
    
    4.Connection to Real Learning
    


# Week 5 Training versus Testing


# Week 6 Theory of Generalization


# Week 7 The VC Dimension


# Week 8 Noise and Error
