
https://www.csie.ntu.edu.tw/~htlin/mooc/

https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf

https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/info

https://www.coursera.org/learn/ntumlone-algorithmicfoundations

# Week 1 The Learning Problem

    1. When machine learning
            1.1 exists some 'underlying pattern to be learned' - so that performance measure can be improved
            
            1.2 but no programmable (easy) definition - so that ML is needed
            
            1.3 somehow there is data about the pattern - so that ML has some inputs to learn from

    2. Why machine learning

    3. How can machine learning

    4. How can machine learn better
    
Components of machine learning
    
    1. Input
    
    2. Output
    
    3. unknown pattern to be learned    
    
    4. data: training examples  
    
    5. hypothesis: skill with hopefully good performance    
    

# Week 2 Learning to Answer Yes/No

    1. Perceptron Hyothesis Set      
    
        Credit Approval Problem: A Simple Hypothesis Set: "The Perceptron"

        线性分类器
    
    2. Perceptron Learning Algorithm (PLA)
    
        选一条最合适的线性分类器

        want g = f on known dataset

        W(t+1) = W(t) + y*x    
    
        利用向量计算,判断正确的perceptron
        
        如果不会停下来??
        
        在一定情况下,这个算法一定会停下来
    
    3. Guarantee of PLA
    
        If PLA halts, D allows some w to make no mistake; call such D linear seperable
        
        Linearly seperable


June 1 2018

    4. Non-seperable Data
    
        PLA: simple to implement, fast, works in any dimension
        
        Assumes linear seperable D to halt; not fully sure how long halting takes (不知道是否线性可分:可能不会停下; 不知道什么时候才会停下)
        
        Learning with Noisy Data : Line with Noise Tolerance
        
        Pocket Algorithm: Modify PLA by keeping best weights in pocket
        
        Since we do no know whether D is linear seperable in advance, we may decide to just go with pocket instead of PLA.
        
        When D is linear seperable: 
        
            pocket on D is slower than PLA for two reasons:
            
                1. pocket要花时间存起来weights
                
                2. 对于每一条线,pocket需要检查所有的点的误差       
               

# Week 3 Types of Learning

    1. Learning with different Output Space
    
        是非题（二元分类）：
        
        • credit approve/disapprove
        • email spam/non-spam
        • patient sick/not sick
        • ad profitable/not profitable
        • answer correct/incorrect (KDDCup 2010)
        
      core and important problem with many tools as building block of other tools
        
      Multiclass Classification Problems
        
            • written digits ⇒ 0, 1, · · · , 9
            • pictures ⇒ apple, orange, strawberry
            • emails ⇒ spam, primary, social, promotion, update (Google)
            
     many applications in practice, especially for ‘recognition’
       
     Regression
       
        • binary classification: patient features ⇒ sick or not
        • multiclass classification: patient features ⇒ which type of cancer
        • regression: patient features ⇒ how many days before recovery
        • Y = R or Y = [lower, upper] ⊂ R (bounded regression)
        —deeply studied in statistics
        • company data ⇒ stock price
        • climate data ⇒ temperature
        
        also core and important with many ‘statistical’ tools as building block of other tools
        
        Structured Learning: Sequence Tagging Problem：
        
        • multiclass classification: word ⇒ word class
        • structured learning: sentence ⇒ structure (class of each word)
        • Y = {PVN, PVP, NVN, PV , · · · }, not including VVVVV
        • huge multiclass classification problem (structure ≡ hyperclass) without ‘explicit’ class definition
        • protein data ⇒ protein folding
        • speech data ⇒ speech parse tree
        
        a fancy but complicated learning problem
        
            1 binary classification
            2 multiclass classification
            3 regression
            4 structured learning

    2. Learning with different data label
    
    3. learning with different protocol
    
    4. learning with different input space
    
# Week 4 Feasibility of Learning

    1. Learning is impossible?
    
    2. Probability to the rescue
    
    3.Connection to Learning
    
    4.Connection to Real Learning
    


# Week 5 Training versus Testing


# Week 6 Theory of Generalization


# Week 7 The VC Dimension


# Week 8 Noise and Error
