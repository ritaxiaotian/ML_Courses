
https://www.csie.ntu.edu.tw/~htlin/mooc/

https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf

https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/info

https://www.coursera.org/learn/ntumlone-algorithmicfoundations

# Week 1 The Learning Problem

    1. When machine learning
            1.1 exists some 'underlying pattern to be learned' - so that performance measure can be improved
            
            1.2 but no programmable (easy) definition - so that ML is needed
            
            1.3 somehow there is data about the pattern - so that ML has some inputs to learn from

    2. Why machine learning

    3. How can machine learning

    4. How can machine learn better
    
Components of machine learning
    
 Â  Â 1. Input
    
 Â  Â 2. Output
    
 Â  Â 3. unknown pattern to be learned    
    
 Â  Â 4. data: training examples  
    
    5. hypothesis: skill with hopefully good performance    
    

# Week 2 Learning to Answer Yes/No

    1. Perceptron Hyothesis Set      
    
        Credit Approval Problem: A Simple Hypothesis Set: "The Perceptron"

        çº¿æ€§åˆ†ç±»å™¨
    
    2. Perceptron Learning Algorithm (PLA)
    
        é€‰ä¸€æ¡æœ€åˆé€‚çš„çº¿æ€§åˆ†ç±»å™¨

        want g = f on known dataset

        W(t+1) = W(t) + y*x    
    
        åˆ©ç”¨å‘é‡è®¡ç®—,åˆ¤æ–­æ­£ç¡®çš„perceptron
        
        å¦‚æœä¸ä¼šåœä¸‹æ¥??
        
        åœ¨ä¸€å®šæƒ…å†µä¸‹,è¿™ä¸ªç®—æ³•ä¸€å®šä¼šåœä¸‹æ¥
    
    3. Guarantee of PLA
    
        If PLA halts, D allows some w to make no mistake; call such D linear seperable
        
        Linearly seperable


June 1 2018

    4. Non-seperable Data
    
        PLA: simple to implement, fast, works in any dimension
        
        Assumes linear seperable D to halt; not fully sure how long halting takes (ä¸çŸ¥é“æ˜¯å¦çº¿æ€§å¯åˆ†:å¯èƒ½ä¸ä¼šåœä¸‹; ä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™æ‰ä¼šåœä¸‹)
        
        Learning with Noisy Data : Line with Noise Tolerance
        
        Pocket Algorithm: Modify PLA by keeping best weights in pocket
        
        Since we do no know whether D is linear seperable in advance, we may decide to just go with pocket instead of PLA.
        
        When D is linear seperable: 
        
            pocket on D is slower than PLA for two reasons:
            
                1. pocketè¦èŠ±æ—¶é—´å­˜èµ·æ¥weights
                
                2. å¯¹äºæ¯ä¸€æ¡çº¿,pocketéœ€è¦æ£€æŸ¥æ‰€æœ‰çš„ç‚¹çš„è¯¯å·®       
               

# Week 3 Types of Learning

    1. Learning with different Output Space
    
        æ˜¯éé¢˜ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰ï¼š
        
        â€¢ credit approve/disapprove
        â€¢ email spam/non-spam
        â€¢ patient sick/not sick
        â€¢ ad profitable/not profitable
        â€¢ answer correct/incorrect (KDDCup 2010)
        
      core and important problem with many tools as building block of other tools
        
      Multiclass Classification Problems
        
            â€¢ written digits â‡’ 0, 1, Â· Â· Â· , 9
            â€¢ pictures â‡’ apple, orange, strawberry
            â€¢ emails â‡’ spam, primary, social, promotion, update (Google)
            
     many applications in practice, especially for â€˜recognitionâ€™
       
     Regression
       
        â€¢ binary classification: patient features â‡’ sick or not
        â€¢ multiclass classification: patient features â‡’ which type of cancer
        â€¢ regression: patient features â‡’ how many days before recovery
        â€¢ Y = R or Y = [lower, upper] âŠ‚ R (bounded regression)
        â€”deeply studied in statistics
        â€¢ company data â‡’ stock price
        â€¢ climate data â‡’ temperature
        
        also core and important with many â€˜statisticalâ€™ tools as building block of other tools
        
        Structured Learning: Sequence Tagging Problemï¼š
        
        â€¢ multiclass classification: word â‡’ word class
        â€¢ structured learning: sentence â‡’ structure (class of each word)
        â€¢ Y = {PVN, PVP, NVN, PV , Â· Â· Â· }, not including VVVVV
        â€¢ huge multiclass classification problem (structure â‰¡ hyperclass) without â€˜explicitâ€™ class definition
        â€¢ protein data â‡’ protein folding
        â€¢ speech data â‡’ speech parse tree
        
        a fancy but complicated learning problem
        
            1 binary classification
            2 multiclass classification
            3 regression
            4 structured learning

    2. Learning with different data label
    
        Supervised: Coin Recognition Revisited
        
        Unsupervised: Coin Recognition without yn
        
            unsupervised multiclass classification â‡â‡’ â€˜clusteringâ€™
            
            Other Clustering Problems
                â€¢ articles â‡’ topics
                â€¢ consumer profiles â‡’ consumer groups
    
        Other Unsupervised Learning Problems
            â€¢ clustering: {x n } â‡’ cluster(x)
            (â‰ˆ â€˜unsupervised multiclass classificationâ€™)
            â€”i.e. articles â‡’ topics
            â€¢ density estimation: {x n } â‡’ density(x)
            (â‰ˆ â€˜unsupervised bounded regressionâ€™)
            â€”i.e. traffic reports with location â‡’ dangerous areas
            â€¢ outlier detection: {x n } â‡’ unusual(x)
            (â‰ˆ extreme â€˜unsupervised binary classificationâ€™)
            â€”i.e. Internet logs â‡’ intrusion alert
            â€¢ . . . and a lot more!!
        unsupervised learning: diverse, with possibly very different performance goals
        
        Semi-supervised: Coin Recognition with Some y n
        
        Other Semi-supervised Learning Problems
            â€¢ face images with a few labeled â‡’ face identifier (Facebook)
            â€¢ medicine data with a few labeled â‡’ medicine effect predictor
            
        semi-supervised learning: leverage unlabeled data to avoid â€˜expensiveâ€™ labeling
        
        Reinforcement Learning
        
            Other Reinforcement Learning Problems Using (x, á»¹ , goodness)
                â€¢ (customer, ad choice, ad click earning) â‡’ ad system
                â€¢ (cards, strategy, winning amount) â‡’ black jack agent
                                
        Learning with Different Data Label y n
            â€¢ supervised: all y n
            â€¢ unsupervised: no y n
            â€¢ semi-supervised: some y n
            â€¢ reinforcement: implicit y n by goodness( á»¹ n )
            â€¢ . . . and more!!
    
    3. learning with different protocol
    
        Batch Learning: Coin Recognition Revisited
        
        batch supervised multiclass classification: learn from all known data
        
            â€¢ batch of (email, spam?) â‡’ spam filter
            â€¢ batch of (patient, cancer) â‡’ cancer classifier
            â€¢ batch of patient data â‡’ group of patients
        
        batch learning: a very common protocol
        
        â€¢ batch spam filter: learn with known (email, spam?) pairs, and predict with fixed g
        â€¢ online spam filter, which sequentially:        
            1 observe an email x t
            2 predict spam status with current g t (x t )
            3 receive â€˜desired labelâ€™ y t from user, and then update g t with (x t , y t )
            
        Connection to What We Have Learned
            â€¢ PLA can be easily adapted to online protocol (how?)
            â€¢ reinforcement learning is often done online (why?)
            
        Protocol â‡” Learning Philosophy
            â€¢ batch: â€˜duck feedingâ€™ unknown target function
            â€¢ online: â€˜passive sequentialâ€™
            â€¢ active: â€˜question askingâ€™ (sequentially)
            â€”query the y n of the chosen xn
    
        Learning with Different Protocol f â‡’ (x n , y n )
            â€¢ batch: all known data unknown target function
            â€¢ online: sequential (passive) data
            â€¢ active: strategically-observed data
            â€¢ . . . and more!!
            
    4. learning with different input space
    
        More on Concrete Features
        
            â€¢ (size, mass) for coin classification
            â€¢ customer info for credit approval
            â€¢ patient info for cancer diagnosis
            â€¢ often including â€˜human intelligenceâ€™ on the learning task
        
        concrete features: the â€˜easyâ€™ ones for ML
        
        Raw Features: Digit Recognition Problem
        
            â€¢ image pixels, speech signal, etc.
        
        raw features: often need human or machines to convert to concrete ones
        
        Abstract Features: Rating Prediction Problem
        
        Rating Prediction Problem (KDDCup 2011)
            â€¢ given previous (userid, itemid, rating) tuples, predict the rating that some userid would give to itemid?
            â€¢ a regression problem with Y âŠ† R as rating and X âŠ† N Ã— N as (userid, itemid)
            â€¢ â€˜no physical meaningâ€™; thus even more difficult for ML Other Problems with Abstract Features
            â€¢ student ID in online tutoring system (KDDCup 2010)
            â€¢ advertisement ID in online ad system

June 2 2018

# Week 4 Feasibility of Learning

    1. Learning is impossible?
    
        all valid reasons, your adversarial teacher can always call you â€˜didnâ€™t learnâ€™. :-(
    
    2. Probability to the rescue
    
        Inferring Something Unknown
        
        Possible versus Probable
        
            does in-sample Î½ say anything about out-of-sample Î¼?
                No!
                possibly not: sample can be mostly green while bin is mostly orange
                
                Yes!
                probably yes: in-sample Î½ likely close to unknown Î¼
                
            in big sample (N large), Î½ is probably close to Î¼ (within )
            
            called Hoeffdingâ€™s Inequality, for marbles, coin, polling, . . .
            
            the statement â€˜Î½ = Î¼â€™ is probably approximately correct (PAC)

June 4

    3.Connection to Learning
    
        Possible versus Probable
        
        Verification of One h
        
        can now use â€˜historical recordsâ€™ (data) to verify â€˜one candidate formulaâ€™ h instead of learning
    
    4.Connection to Real Learning
    
        BAD sample: E in and E out far away can get worse when involving â€˜choiceâ€™
        
        BAD data for many h
        â‡â‡’ no â€˜freedom of choiceâ€™ by A
        â‡â‡’ there exists some h such that E out (h) and E in (h) far away


# Week 5 Training versus Testing


# Week 6 Theory of Generalization


# Week 7 The VC Dimension


# Week 8 Noise and Error
